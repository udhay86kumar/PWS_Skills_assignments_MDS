{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1srLEOcPvo4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is the KNN Algorithm?\n",
        "\n",
        "The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric, supervised learning algorithm used for both classification and regression tasks. It works by finding the \"k\" closest training data points (neighbors) to a new data point and predicting its label based on the majority label (for classification) or average value (for regression) of those neighbors. It relies heavily on distance metrics (e.g., Euclidean, Manhattan) to determine the proximity between data points.\n"
      ],
      "metadata": {
        "id": "K0x4o_ZvPwd2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "93wBQZJ9P8Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q2. How Do You Choose the Value of K in KNN?\n",
        "Choosing the right value of K is crucial, as it directly impacts the model’s performance:\n",
        "\n",
        "Cross-Validation: Perform cross-validation to evaluate different K values and select the one that yields the best accuracy.\n",
        "Bias-Variance Tradeoff: Smaller K values may lead to a highly flexible model, risking overfitting, while larger K values can lead to underfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "7cT0I1JcPwkz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Kzmz9CWQ4yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. Difference Between KNN Classifier and KNN Regressor\n",
        "\n",
        "KNN Classifier: Used for classification tasks where the model predicts the class label of a new data point based on the majority class of its K-nearest neighbors.\n",
        "KNN Regressor: Used for regression tasks where the model predicts a continuous value by averaging the target values of the K-nearest neighbors.\n"
      ],
      "metadata": {
        "id": "OueDzYmWPwrN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3J8T-zGQ4Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. How Do You Measure the Performance of KNN?\n",
        "For Classification: Use metrics like accuracy, precision, recall, F1-score, and the confusion matrix.\n",
        "For Regression: Evaluate with metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (R²).\n"
      ],
      "metadata": {
        "id": "AfLJ9QEgPwuS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XxMF25MhQ33F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. What is the Curse of Dimensionality in KNN?\n",
        "The curse of dimensionality refers to the phenomenon where the distance between data points becomes less meaningful as the number of features (dimensions) increases. In high-dimensional spaces, the distance between any two points becomes nearly identical, making it difficult for KNN to effectively determine the nearest neighbors. This can lead to reduced model accuracy and increased computational complexity.\n",
        "\n"
      ],
      "metadata": {
        "id": "TzRIXw8kPwxF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HKKQ766UQ3bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. How Do You Handle Missing Values in KNN?\n",
        "Imputation: Replace missing values with the mean, median, or mode of the feature.\n",
        "KNN Imputation: Fill missing values by finding the K-nearest neighbors and taking the average (for continuous data) or majority value (for categorical data).\n",
        "Dropping Records: In cases with few missing values, you may drop records with missing entries if it does not affect data integrity.\n"
      ],
      "metadata": {
        "id": "fekqu0QzQEcI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u66Moy8xQ293"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. Compare and Contrast the Performance of the KNN Classifier and Regressor\n",
        "KNN Classifier: Works well for problems where the decision boundaries are simple and where data classes are well-separated. It’s suitable for applications like image classification and text categorization.\n",
        "KNN Regressor: Used in regression problems where target values are continuous. It works best when target values of nearby points are correlated, such as in predictive analysis for house prices.\n"
      ],
      "metadata": {
        "id": "bE-C7L00QEe5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKclkSaOQ2hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. Strengths and Weaknesses of the KNN Algorithm for Classification and Regression Tasks\n",
        "Strengths:\n",
        "\n",
        "Simplicity: KNN is easy to understand and implement.\n",
        "Non-parametric: No assumptions about data distribution are required.\n",
        "Versatile: Can be used for both classification and regression.\n",
        "Weaknesses:\n",
        "\n",
        "Computationally Intensive: KNN is slow during prediction, especially with large datasets, as it calculates distances for each new data point.\n",
        "Sensitive to Irrelevant Features: Irrelevant or redundant features can affect distance calculations and model accuracy.\n",
        "Ineffective in High Dimensions: Suffers from the curse of dimensionality.\n",
        "\n",
        "Addressing Weaknesses:\n",
        "\n",
        "Use dimensionality reduction techniques (e.g., PCA).\n",
        "Perform feature selection to retain only the most relevant features.\n",
        "Implement algorithms like KD-Tree or Ball-Tree for faster distance calculations.\n"
      ],
      "metadata": {
        "id": "4H863YXCQEhs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H7PHDPgwQ10-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. Difference Between Euclidean Distance and Manhattan Distance in KNN\n",
        "Euclidean Distance: Measures the straight-line (or \"as-the-crow-flies\") distance between points in space. It's suitable for continuous, numerical data and when the distance in all directions is equally significant.\n",
        "\n",
        "\n",
        "\n",
        "Manhattan Distance: Measures the distance between points by only allowing vertical and horizontal moves (like a grid or city block). It's often used when dimensions are not correlated and is less sensitive to outliers than Euclidean distance.\n",
        "\n",
        "Formula :\n",
        "<img src = \"https://th.bing.com/th/id/OIP.kYpQBUb08mLQi80C3QevwwHaDK?w=350&h=149&c=7&r=0&o=5&dpr=1.4&pid=1.7\">\n",
        "\n"
      ],
      "metadata": {
        "id": "rm_zVT2yQEke"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2TxVhBOTQ1Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10. Role of Feature Scaling in KNN\n",
        "Feature scaling is crucial in KNN because the algorithm relies on distance calculations to determine neighbors. Features with larger ranges can dominate distance metrics, skewing the algorithm’s results. Applying techniques like min-max normalization or standardization ensures all features contribute equally to the distance calculation, leading to more accurate predictions."
      ],
      "metadata": {
        "id": "JeTQ-A_oQEmu"
      }
    }
  ]
}