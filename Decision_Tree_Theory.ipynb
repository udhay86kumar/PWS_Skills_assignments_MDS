{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ef40d4",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier - Theoretical Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d64f6",
   "metadata": {},
   "source": [
    "## Q1. Describe the decision tree classifier algorithm and how it works to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3674e",
   "metadata": {},
   "source": [
    "A decision tree is a supervised learning algorithm used for classification and regression tasks. It works by splitting the data into subsets based on feature values, using a tree-like structure. At each node, the algorithm selects the best feature to split on using metrics like Gini impurity or entropy. Predictions are made by traversing the tree from the root to a leaf node based on feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba672c2",
   "metadata": {},
   "source": [
    "## Q2. Step-by-step mathematical intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f1abe",
   "metadata": {},
   "source": [
    "1. **Choosing the Best Split**: The algorithm selects the feature that best separates the classes.\n",
    "   - **Gini Impurity**: Measures how often a randomly chosen element would be incorrectly classified.\n",
    "     \\[ Gini = 1 - \\sum (p_i^2) \\]\n",
    "   - **Entropy (Information Gain)**: Measures disorder in the dataset.\n",
    "     \\[ Entropy = -\\sum p_i \\log_2(p_i) \\]\n",
    "2. **Splitting Criteria**: The algorithm selects the feature with the highest information gain or lowest Gini impurity.\n",
    "3. **Stopping Criteria**: The tree stops growing when further splits do not improve classification accuracy or a depth limit is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd150f1",
   "metadata": {},
   "source": [
    "## Q3. How a decision tree classifier solves a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4d128",
   "metadata": {},
   "source": [
    "A decision tree classifier solves binary classification by recursively splitting the dataset into two groups at each node based on a feature's threshold. The process continues until pure (or nearly pure) leaf nodes are reached. Each leaf node represents a class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5a910",
   "metadata": {},
   "source": [
    "## Q4. Geometric intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa11c5",
   "metadata": {},
   "source": [
    "Decision trees create axis-aligned decision boundaries in the feature space. Each split divides the space into regions where all points within a region belong to the same class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a76b53",
   "metadata": {},
   "source": [
    "## Q5. What is a confusion matrix and how it evaluates classification models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee284a8",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that summarizes the performance of a classification model:\n",
    "\n",
    "| Actual \\ Predicted | Positive | Negative |\n",
    "|--------------------|----------|----------|\n",
    "| **Positive**       | TP       | FN       |\n",
    "| **Negative**       | FP       | TN       |\n",
    "\n",
    "- **True Positives (TP)**: Correctly predicted positive instances.\n",
    "- **False Positives (FP)**: Incorrectly predicted positive instances.\n",
    "- **False Negatives (FN)**: Incorrectly predicted negative instances.\n",
    "- **True Negatives (TN)**: Correctly predicted negative instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce422728",
   "metadata": {},
   "source": [
    "## Q6. Example of a confusion matrix and how to calculate precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b686d",
   "metadata": {},
   "source": [
    "Consider the following confusion matrix for a binary classification problem:\n",
    "\n",
    "| Actual \\ Predicted | Positive | Negative |\n",
    "|--------------------|----------|----------|\n",
    "| **Positive**       | 50       | 10       |\n",
    "| **Negative**       | 5        | 35       |\n",
    "\n",
    "- **Precision** (Positive Predictive Value): \\( \\frac{TP}{TP + FP} = \\frac{50}{50 + 5} = 0.91 \\)\n",
    "- **Recall** (Sensitivity): \\( \\frac{TP}{TP + FN} = \\frac{50}{50 + 10} = 0.83 \\)\n",
    "- **F1-Score** (Harmonic mean of precision and recall): \\( 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} = 0.87 \\)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938b83c",
   "metadata": {},
   "source": [
    "## Q7. Importance of choosing the right evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3ce76",
   "metadata": {},
   "source": [
    "Different classification problems require different evaluation metrics. For imbalanced datasets, accuracy can be misleading, so precision, recall, or F1-score should be considered. In cases like spam detection, false positives and false negatives have different costs, influencing the choice of metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e13fd",
   "metadata": {},
   "source": [
    "## Q8. Example where precision is the most important metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273e0e2",
   "metadata": {},
   "source": [
    "In spam email detection, precision is crucial because false positives (misclassifying legitimate emails as spam) are more problematic than false negatives (a spam email not being filtered)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df784305",
   "metadata": {},
   "source": [
    "## Q9. Example where recall is the most important metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae63fa",
   "metadata": {},
   "source": [
    "In medical diagnosis (e.g., cancer detection), recall is crucial since false negatives (failing to detect cancer) can have severe consequences."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}